{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKscBXaBsTUy",
    "outputId": "10c69faa-adb6-4b33-8020-b317b7f30d0e"
   },
   "outputs": [],
   "source": [
    "!pip -U install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236,
     "referenced_widgets": [
      "465dc33802ec4bc29e4c243afba57049",
      "9024fdd7bb5040c38584c307fedbac39",
      "7106c338cd3f4b2092a4daa045fe8ff0",
      "bccdbfbd351f42fb8a4684d31de79aaf",
      "3d1fbea6384d41aa8ce23d0d17d972f9",
      "7a08a5fdb5f3444bbf69b2d64c968845",
      "ae5c541b34a9402591e6ad2ab7f7eb33",
      "0616c5d862394282a5684e1563633624",
      "c456952e31ae40789541b13ae2289674",
      "90f647850677415287a494c644b65e0f",
      "016e1445593d4fdb98a62b6ae750fb39",
      "dce931c3ce9f4f9fb6a04ec78ec4e8af",
      "80d789b5765041edbc91131798d8de78",
      "3361f7ceadda4fe2bf4b333928133686",
      "1f809801ae06477eb30274b8c1b1deb9",
      "77cd75a4dae4477ca4c16369afc039f2",
      "82d27244a8e84949a95b1aefa266721b",
      "bfe53d9cb77a4faba6cb68cb6209c1e1",
      "10b00022875849499e0bb7a55bc952b0",
      "c2dd13c9f9eb4133b6c0ec710bf0eccc",
      "a9beff385a1648368d06caa2ab61bf26",
      "f6e0636ba24a48e485d25db4e40c19b0",
      "b6d18eaff9544c249c95d3f6b1f66bbd",
      "851dd054e25a40608ec93a9fd8bb4d63",
      "67f05fdb9ae64d2dbf628e0556799287",
      "c316b52c7f374ab9b7a6fea956d1dda8",
      "96b207da54ff44a68454740a52d9d14b",
      "31c66c5cfa19428fa7474b1bc30e342f",
      "ef3025bfc1e24248bc7a026120dafb84",
      "647b4cba357249829d9fc2ba37876808",
      "23b64da174f34912b94d8ad62c94bb73",
      "b5b65c6687684299abecd80b127b41cd",
      "e99f23a83324432d934f3bcd9313c1e9",
      "cd97bc7b9b3a402aab5799d5145b08f4",
      "8ec9e9ac88e0498aa3389a583a54a918",
      "d805f238953c4d2dac85417cfe4c4789",
      "70f0afa213fd4e4496049ce9a95dd75a",
      "90454ed5346847fba3e77b597427136e",
      "9c74e02cbc28471d994e27ec3cadee9d",
      "dccf7655bab143f094f9a0604fa55812",
      "58f22d626a76400b98ac244dd0df2df4",
      "12e78497c6e543ecaa84976f7f305790",
      "7f52340fb1f74f99a5c959f31e363b57",
      "8f71851172dc4366bbdc17a9b70b8a32"
     ]
    },
    "id": "DB3ngsDRtequ",
    "outputId": "746f3954-cf8c-48a2-f6f0-151ad264c256"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"sentiment-analysis\")\n",
    "pipe(\"it is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjxrYH6YuAve"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_model():\n",
    "    model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4K0CEUtzupxy"
   },
   "outputs": [],
   "source": [
    "from model.load_model import load_model\n",
    "import torch\n",
    "\n",
    "tokenizer, model = load_model()\n",
    "\n",
    "def get_response(user_input):\n",
    "    input_ids = tokenizer.encode(user_input, return_tensors=\"pt\").cuda()\n",
    "    output = model.generate(input_ids, max_new_tokens=150, do_sample=True)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZJ0FQ_OuwO0"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def store_feedback(question, answer, feedback):\n",
    "    if feedback.lower() == \"negative\":\n",
    "        with open(\"feedback/feedback_data.csv\", \"a\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([question, answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-CmvhCIu6_e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def get_dataset():\n",
    "    df = pd.read_csv(\"feedback/feedback_data.csv\", names=[\"question\", \"answer\"])\n",
    "    df[\"text\"] = df.apply(lambda row: f\"User: {row['question']} \\nAI: {row['answer']}\", axis=1)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxdWw7o1u_DZ"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from model.load_model import load_model\n",
    "from training.prepare_dataset import get_dataset\n",
    "\n",
    "def fine_tune_on_feedback():\n",
    "    tokenizer, model = load_model()\n",
    "    dataset = get_dataset()\n",
    "\n",
    "    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=16, lora_dropout=0.1, bias=\"none\")\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"./checkpoints\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        num_train_epochs=2,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mT2kdsT7vB5c"
   },
   "outputs": [],
   "source": [
    "from inference.chat import get_response\n",
    "from feedback.store_feedback import store_feedback\n",
    "\n",
    "while True:\n",
    "    question = input(\"You: \")\n",
    "    if question.lower() == \"exit\":\n",
    "        break\n",
    "    answer = get_response(question)\n",
    "    print(\"AI:\", answer)\n",
    "    feedback = input(\"Was the answer helpful? (positive/negative): \")\n",
    "    store_feedback(question, answer, feedback)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
